Ambari cluster implementation has done in AWS using ec2 instances got struck at end after 90 % work done

screen shots has been already sent through email

Below are tasks of hive ,Sqoop ,node decommission and commission.

=============================
create external table consumercomplaints(
Date_received string,
Product string,
Sub_product string,
Issue string,
Sub_issue string,
Consumer_complaint_narrative string,
Company_public_response string,
Company	 string,
State string,
ZIP_code string,
Tags string,
Consumer_consent_provided string,
Submitted_via string,
Date_sent_to_company string,
Company_response_to_consumer string,
Timely_response string,
Consumer_disputed string,
Complaint_ID string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
LINES TERMINATED BY "\n"
LOCATION '/user/devuser/test';



create external table consumercomplaints_partitioned(
Date_received string,
Product string,
Sub_product string,
Issue string,
Sub_issue string,
Consumer_complaint_narrative string,
Company_public_response string,
Company	 string,
ZIP_code string,
Tags string,
Consumer_consent_provided string,
Submitted_via string,
Date_sent_to_company string,
Company_response_to_consumer string,
Timely_response string,
Consumer_disputed string,
Complaint_ID string)
partitioned by (state string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
LINES TERMINATED BY '\n'
STORED AS TEXTFILE
LOCATION '/user/devuser/test2';


loaded data into consumercomplaints table
load data local inpath '/home/devuser/swagat/consumercomplaints/Consumer_Complaints.csv' into table consumercomplaints;

inserting data into partitioned table
insert into table consumercomplaints_partitioned partition (state) select * from consumercomplaints;

Loading data from temp table to partition table:
used Query:
insert into table consumercomplaints_partitioned partition (state) select Date_received ,Product ,Sub_product ,Issue ,Sub_issue ,Consumer_complaint_narrative ,Company_public_response ,Company ,ZIP_code ,Tags ,Consumer_consent_provided ,Submitted_via ,Date_sent_to_company ,Company_response_to_consumer ,Timely_response ,Consumer_disputed ,Complaint_ID ,State from consumercomplaints where state not like '%/201%';

==================================================

select company,count(Complaint_ID) from consumercomplaints_partitioned group by company;


select count(Consumer_disputed) from consumercomplaints_partitioned group by Consumer_disputed;

select case when Consumer_disputed = 'Yes' then 'disputed' when Consumer_disputed = 'No' then 'undisputed' end,count(Consumer_disputed) from consumercomplaints_partitioned where Consumer_disputed != 'NULL' group by Consumer_disputed;


select * from consumercomplaints_partitioned where Product like '%Mortgage%' and Issue like '%foreclosure%' and Company like '%Bank of America%';




create external table consumercomplaints_new(
Date_received string,
Product string,
Sub_product string,
Issue string,
Sub_issue string,
Consumer_complaint_narrative string,
Company_public_response string,
Company	 string,
State string,
ZIP_code string,
Tags string,
Consumer_consent_provided string,
Submitted_via string,
Date_sent_to_company string,
Company_response_to_consumer string,
Timely_response string,
Consumer_disputed string,
Complaint_ID string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
LINES TERMINATED BY "\n"
LOCATION '/user/devuser/test3';


load data local inpath '/home/devuser/swagat/consumercomplaints/Consumer_Complaints.csv' into table consumercomplaints_new;

create table consumercomplaints_new_States like consumercomplaints_new;

insert into table consumercomplaints_new_States select * from consumercomplaints_new where state like 'NC' or state like 'TX';

insert overwrite table consumercomplaints_new select * from consumercomplaints_new where state not like 'NC' or state not like 'TX';

insert overwrite table consumercomplaints_new select * from consumercomplaints_new_states where state like 'NC';

if only insert 

insert into table consumercomplaints_new select * from consumercomplaints_new_states where state like 'NC';


==========================================

Data node Commission and decommission:

The first task is to update the ‘exclude‘ files for both HDFS (hdfs-site.xml) and MapReduce (mapred-site.xml).

The ‘exclude’ file:

for jobtracker contains the list of hosts that should be excluded by the jobtracker. If the value is empty, no hosts are excluded.
for Namenode contains a list of hosts that are not permitted to connect to the Namenode.
Here is the sample configuration for the exclude file in hdfs-site.xml and mapred-site.xml:

hdfs-site.xml

<property>
<name>dfs.hosts.exclude</name>
<value>/home/hadoop/excludes</value>
<final>true</final>
</property>

mapred-site.xml

<property>
<name>mapred.hosts.exclude</name>
<value>/home/hadoop/excludes</value>
<final>true</final>
</property>

Similarly, we have the ‘include’ files:
for jobtracker containing the list of nodes that may connect to the JobTracker. If the value is empty, all hosts are permitted.
for Namenode containing a list of hosts that are permitted to connect to the Namenode. If the value is empty, all hosts are permitted.
The ‘dfsadmin’ and ‘mradmin’ commands refresh the configuration with the changes to make them aware of the new node.

The ‘slaves’ file on master server contains the list of all data nodes. This must also be updated to ensure any issues in future hadoop daemon start/stop


The important step in data node commission process is to run the Cluster Balancer.

>hadoop balancer -threshold 40

Balancer attempts to provide a balance to a certain threshold among data nodes by copying block data from older nodes to newly commissioned nodes.

So, this is how you can do – Commissioning and Decommissioning Nodes in a Hadoop Cluster.


======================
Increasing Java Heap size:

Following are the recommended settings for HADOOP_NAMENODE_OPTS in the hadoop-env.sh file (replacing the ##### placeholder for -XX:NewSize, -XX:MaxNewSize, -Xms, and -Xmx with the recommended values from the table):

-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile=/var/log/hadoop/$USER/hs_err_pid%p.log -XX:NewSize=##### -XX:MaxNewSize=##### -Xms##### -Xmx##### -XX:PermSize=128m -XX:MaxPermSize=256m -Xloggc:/var/log/hadoop/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT ${HADOOP_NAMENODE_OPTS}


If the cluster uses a secondary NameNode, you should also set HADOOP_SECONDARYNAMENODE_OPTS to HADOOP_NAMENODE_OPTS in the hadoop-env.sh file:

HADOOP_SECONDARYNAMENODE_OPTS=$HADOOP_NAMENODE_OPTS

Another useful HADOOP_NAMENODE_OPTS setting is -XX:+HeapDumpOnOutOfMemoryError. This option specifies that a heap dump should be executed when an out-of-memory error occurs. You should also use -XX:HeapDumpPath to specify the location for the heap dump file:

-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=./etc/heapdump.hprof

=================================


Falcon service used for changing the data replication
to start this service we need to login to ambari UI and add the service and install the service in all the nodes
once after setting up all the things need to restart all services

==================================

$ sqoop import \
--connect jdbc:mysql://hadooptest.cyrjpmsirks6.us-east-1.rds.amazonaws.com/AdventureWorks \
--username HadoopTest -p CanUSq00p1T?! 
--table AdventureWorks
--hcatalog-table AdventureWorks
-m 4


select Product,sum(sales) from AdventureWorks where ProductModel = 'Racing Socks' group by Product;









